{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvBrJtc2WVGCDPTpn+pWjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bb426/PyTorch-tutorials-kr/blob/master/torch_tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1PYA3Lp6kpZ"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcqZDR8n6-pc"
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzt_Hrl-7GxR"
      },
      "source": [
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmrzO-n7wCy"
      },
      "source": [
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JxHxCLE8I_g"
      },
      "source": [
        "learning_rate = 1e-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb6mv9s58KE8",
        "outputId": "316f3729-4700-4726-cab7-60b121923ac9"
      },
      "source": [
        "for t in range(10):\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(f'{t} epoch, {loss:.0f}')\n",
        "\n",
        "    # backprop\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "    # update\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch, 211\n",
            "1 epoch, 199\n",
            "2 epoch, 187\n",
            "3 epoch, 175\n",
            "4 epoch, 165\n",
            "5 epoch, 155\n",
            "6 epoch, 146\n",
            "7 epoch, 137\n",
            "8 epoch, 129\n",
            "9 epoch, 121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hi8gyH59uUm",
        "outputId": "e5bdbd5a-a23c-4066-c5d6-084ad51ed86f"
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cuda')\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(10):\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(f'{t} epoch, {loss:.0f}')\n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # update\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch, 33364364\n",
            "1 epoch, 31433824\n",
            "2 epoch, 33470514\n",
            "3 epoch, 33996788\n",
            "4 epoch, 29262458\n",
            "5 epoch, 20299608\n",
            "6 epoch, 11580825\n",
            "7 epoch, 5996296\n",
            "8 epoch, 3186441\n",
            "9 epoch, 1896194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f72H14zdC7MH",
        "outputId": "72dcfdcc-971d-4c48-d2db-f8722582eee8"
      },
      "source": [
        "import torch\n",
        "torch.device('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD_NEf_LDITB",
        "outputId": "bce33bdd-686c-44bd-fe69-41d22b2a1440"
      },
      "source": [
        "# autograd\n",
        "\n",
        "import torch\n",
        "dtype = torch.float\n",
        "device = torch.device('cuda')\n",
        "\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for i in range(10):\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "    print(f'{t} epoch, {loss:.0f}')\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 epoch, 31170508\n",
            "9 epoch, 23608798\n",
            "9 epoch, 18951146\n",
            "9 epoch, 14871194\n",
            "9 epoch, 11050088\n",
            "9 epoch, 7809904\n",
            "9 epoch, 5345988\n",
            "9 epoch, 3635232\n",
            "9 epoch, 2504482\n",
            "9 epoch, 1774824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PZDcgipHmXo"
      },
      "source": [
        "# autograd with a forward function\n",
        "\n",
        "import torch\n",
        "\n",
        "class MyReLU(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Built custom autograd function\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(input):\n",
        "        \"\"\"\n",
        "        input tensor를 받아 출력 tensor return\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        # relu\n",
        "        return input.clamp(min=0) \n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        gradients를 return\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensor()\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input < 0] = 0\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cpu')\n",
        "\n",
        "for i in range(10):\n",
        "    relu = MyReLU.apply\n",
        "\n",
        "    y_pred = relu(x.mm(w1)).mm(w2)\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "\n",
        "    print(f'{i} epoch, {loss:.0f}')\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZuC3Sq3L4rl",
        "outputId": "cdb0f30f-4675-4640-9fd8-f138e4c1f3f3"
      },
      "source": [
        "# backprop using nn Module\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H, bias=False),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out, bias=False)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "learning_rate = 1e-3\n",
        "\n",
        "for i in range(10):\n",
        "    y_pred = model(x)  ## nn 모듈은 입력 tensor를 모듈에 직접 전달하여 출력 tensor 생성할 수 있음 (__call__ 연산자를 override 해서 그렇다고 함)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    print(f'{i} epoch, {loss:.0f}')\n",
        "\n",
        "    # 먼저 gradient 초기화\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # gradient 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters(): #model.paramemters()에 있는 parameters는 모두 tensor이므로 gradient에 접근 가능함\n",
        "            param -= learning_rate * param.grad\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch, 1\n",
            "1 epoch, 1\n",
            "2 epoch, 1\n",
            "3 epoch, 1\n",
            "4 epoch, 1\n",
            "5 epoch, 1\n",
            "6 epoch, 1\n",
            "7 epoch, 1\n",
            "8 epoch, 1\n",
            "9 epoch, 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8h-MEcPPn-P",
        "outputId": "78e6b3e3-2e88-4eda-c70a-76cc48668aa4"
      },
      "source": [
        "# optim package\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(10):\n",
        "    y_pred = model(x)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(f'{i} epoch, {loss:.0f}')\n",
        "\n",
        "    optimizer.zero_grad() #gradients 초기화, loss.backward()는 덮어쓰는게 아니라 += 형태라 반드시 초기화해줘야 함\n",
        "    loss.backward() #gradient 계산\n",
        "    optimizer.step() #update\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch, 658\n",
            "1 epoch, 641\n",
            "2 epoch, 624\n",
            "3 epoch, 608\n",
            "4 epoch, 592\n",
            "5 epoch, 576\n",
            "6 epoch, 561\n",
            "7 epoch, 546\n",
            "8 epoch, 532\n",
            "9 epoch, 518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJfbUL0KRJOk",
        "outputId": "02c3ebe7-5088-4794-911a-714cd4d5b78b"
      },
      "source": [
        "# nn.Module\n",
        "# Sequencial 보다 더 복잡한 모델을 구성해야 할 경우\n",
        "\n",
        "class TwoLayerNet(torch.nn.Module):\n",
        "    # 새 모듈을 정의\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n",
        "for i in range(10):\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(i, ' th iter', loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  th iter 667.561279296875\n",
            "1  th iter 617.8712158203125\n",
            "2  th iter 574.3640747070312\n",
            "3  th iter 536.1549072265625\n",
            "4  th iter 502.1356506347656\n",
            "5  th iter 471.53076171875\n",
            "6  th iter 443.6327209472656\n",
            "7  th iter 418.0291748046875\n",
            "8  th iter 394.4185791015625\n",
            "9  th iter 372.456787109375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFHB7lsT7k2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}